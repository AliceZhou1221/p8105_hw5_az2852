---
title: "p8105_hw5_az2852"
output: github_document
---

```{r setup, message = FALSE}
library(tidyverse)
library(rvest)
library(broom)
```

# Problem 1
Suppose you put n people in a room, and want to know the probability that at least two people share a birthday. For simplicity, we’ll assume there are no leap years (i.e. there are only 365 days) and that birthdays are uniformly distributed over the year (which is actually not the case).

```{r}
sample = sample(1:365, size = 10, replace = TRUE)

length(unique(sample)) < 10
```

write a function
```{r}
bday_sim = function(n) {
  
  sample = sample(1:365, size = n, replace = TRUE)

duplicates = length(unique(sample)) < n

return(duplicates)
}

bday_sim(40)
```
run this function 10000 times for each group size between 2 and 50. For each group size, compute the probability that at least two people in the group will share a birthday by averaging across the 10000 simulation runs. 

```{r}
bday_sim = expand_grid(
    iter = 1:10000,
    n = 2:50) %>% 
  mutate(res = map_lgl(n, bday_sim)) %>% 
  group_by(n) %>% 
  summarize(
    prob = mean(res)
  )

```
Make a plot showing the probability as a function of group size, and comment on your results.
```{r}
ggplot(bday_sim, aes(x = n, y = prob))+
  geom_line()
```
Problem 2
When designing an experiment or analysis, a common question is whether it is likely that a true effect will be detected – put differently, whether a false null hypothesis will be rejected. The probability that a false null hypothesis is rejected is referred to as power, and it depends on several factors, including: the sample size; the effect size; and the error variance. In this problem, you will conduct a simulation to explore power in a one-sample t-test.

First set the following design elements:

Fix 𝑛=30
Fix 𝜎=5
Set 𝜇=0
Generate 5000 datasets from the model

𝑥∼𝑁𝑜𝑟𝑚𝑎𝑙[𝜇,𝜎]

For each dataset, save 𝜇̂ 
 and the p-value arising from a test of 𝐻:𝜇=0
 using 𝛼=0.05
```{r}
generate_normal_fixed = function(n = 30, mean = 0, sd = 5) {
  norm_vec = rnorm(n = n, mean = mean, sd = sd)
  return (norm_vec)
}
generate_normal_fixed()

# Generate 5000 datasets
fixed_sim_df = tibble(
  iter = 1:5000,
  data = map(1:5000, ~generate_normal_fixed(n = 30, mean = 0, sd = 5))) %>% 
  mutate(
    t_test_res = map(data, \(x) t.test(x, mu = 0)),
    tidy_res = map(t_test_res, tidy)
  ) %>% 
  unnest(tidy_res) %>%
  select(iter, estimate, p.value)

fixed_sim_df
```

Repeat the above for 𝜇={1,2,3,4,5,6}
```{r}
generate_normal = function(n = 30, pop_mean, sd = 5) {
  norm_vec = rnorm(n = n, mean = pop_mean, sd = sd)
  return (norm_vec)
}
generate_normal(pop_mean = 6)

sim_df = expand_grid(
    iter = 1:5,
    pop_mean = c(1,2,3,4,5,6)) %>% 
    mutate(
    data = map(pop_mean, \(x) generate_normal(n = 30, x, sd = 5)),
    t_test_res = map(data, \(x) t.test(x, mu = 0)),
    tidy_res = map(t_test_res, tidy)
  ) %>% 
  unnest(tidy_res) %>% 
  select(iter, pop_mean, estimate, p.value)

#mean(sim_df[["data"]][[1]])
#mean(sim_df[["data"]][[2]])
#mean(sim_df[["data"]][[3]])
#mean(sim_df[["data"]][[4]])
#mean(sim_df[["data"]][[5]])
#mean(sim_df[["data"]][[6]])
#mean(sim_df[["data"]][[7]])
```

```{r}
generate_normal = function(n = 30, pop_mean, sd = 5) {
  norm_vec = rnorm(n = n, mean = pop_mean, sd = sd)
  return (norm_vec)
}
generate_normal(pop_mean = 6)

sim_df =
  expand_grid(
    iter = 1:5000,
    pop_mean = c(1,2,3,4,5,6)) %>% 
    mutate(
    data = map(pop_mean, \(x) generate_normal(n = 30, x, sd = 5)),
    t_test_res = map(data, \(x) t.test(x, mu = 0)),
    tidy_res = map(t_test_res, tidy)
  ) %>% 
  unnest(tidy_res) %>%
  select(iter, estimate, p.value)
```


, and complete the following:

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of 𝜇on the x axis. Describe the association between effect size and power.
Make a plot showing the average estimate of 𝜇̂ 
 on the y axis and the true value of 𝜇
 on the x axis. Make a second plot (or overlay on the first) the average estimate of 𝜇̂ 
 only in samples for which the null was rejected on the y axis and the true value of 𝜇
 on the x axis. Is the sample average of 𝜇̂ 
 across tests for which the null is rejected approximately equal to the true value of 𝜇
? Why or why not?

